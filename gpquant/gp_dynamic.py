from gplearn.genetic import SymbolicRegressor

from gplearn.fitness import make_fitness

from sklearn.utils import check_random_state

import numpy as np
import pylab as plt


class DynamicSymbolicRegressor(SymbolicRegressor):
    def __init__(self,
                 metric,
                 population_size=1000,
                 generations=20,
                 tournament_size=20,
                 stopping_criteria=0.0,
                 const_range=(-1., 1.),
                 init_depth=(2, 6),
                 init_method='half and half',
                 function_set=('add', 'sub', 'mul', 'div'),
                 parsimony_coefficient=0.001,
                 p_crossover=0.9,
                 p_subtree_mutation=0.01,
                 p_hoist_mutation=0.01,
                 p_point_mutation=0.01,
                 p_point_replace=0.05,
                 max_samples=1.0,
                 warm_start=False,
                 n_jobs=1,
                 verbose=0,
                 random_state=None):
        super(DynamicSymbolicRegressor, self).__init__(
            population_size=population_size,
            generations=generations,
            tournament_size=tournament_size,
            stopping_criteria=stopping_criteria,
            const_range=const_range,
            init_depth=init_depth,
            init_method=init_method,
            function_set=function_set,
            metric=metric,
            parsimony_coefficient=parsimony_coefficient,
            p_crossover=p_crossover,
            p_subtree_mutation=p_subtree_mutation,
            p_hoist_mutation=p_hoist_mutation,
            p_point_mutation=p_point_mutation,
            p_point_replace=p_point_replace,
            max_samples=max_samples,
            warm_start=warm_start,
            n_jobs=n_jobs,
            verbose=verbose,
            random_state=random_state)

    @staticmethod
    def make_explict_fitness(func, metric, greater_is_better, use_raw_y=False):
        """

        :param func: function
            the function this is used to get the reward given output
        :param metric: function
            the function measures the fitness
        :param greater_is_better: bool
            whether it is true that the greater the fitness is the better the performance is
        :return:
        """

        def _fitness(y, y_pred, sample_weight):
            """
               :param y: [0] * len(x)
                   This should be None since we wont know the reward before we get the y_pred.
                   In practice we use [0] * len(x)
               :param y_pred:
                   The y_pred generated by the algorithm
               :param sample_weight:
                   sample_weight for each label
               :return:
            """
            if use_raw_y:
                y = func(y)
            else:
                y = func(y_pred)
            return metric(y, y_pred, sample_weight)

        return make_fitness(_fitness, greater_is_better)

    def dynamic_fit(self, X):
        _ = [i for i in range(X.shape[0])]
        super(DynamicSymbolicRegressor, self).fit(X, _)
